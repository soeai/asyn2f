{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.datasets import cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data separately\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train from 4D to 2D array (number of samples, width*height*channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "# Reshape y_train to 1D array\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((x_train, y_train))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "        85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3073)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the training dataset by category\n",
    "data_by_class = {i: [] for i in range(100)}\n",
    "for row in train_data:\n",
    "    data_by_class[int(row[-1])].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6300, 6100, 5700, 5700, 4900, 4800, 4700, 4600, 4500, 2700]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate proportions for 10 chunks using a normal distribution around the mean\n",
    "mean = 0.05\n",
    "std_dev = 0.01\n",
    "proportions = np.random.normal(mean, std_dev, 10)\n",
    "\n",
    "# Clip values to ensure they're in range [0,1] and normalize so they sum to 1\n",
    "proportions = np.clip(proportions, 0, 1)\n",
    "proportions /= np.sum(proportions)\n",
    "\n",
    "# Compute the chunk sizes based on proportions, and ensure they're multiples of 10\n",
    "chunk_sizes = [int(p * 50000) // 100 * 100 for p in proportions]\n",
    "\n",
    "# Adjust the last chunk size to ensure the total sum is 50000\n",
    "chunk_sizes[-1] = 50000 - sum(chunk_sizes[:-1])\n",
    "\n",
    "# Sorting in descending order using sorted()\n",
    "chunk_sizes = sorted(chunk_sizes, reverse=True)\n",
    "chunk_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(chunk_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 chunks with specified sizes and IID distribution\n",
    "chunks = []\n",
    "for size in chunk_sizes:\n",
    "    chunk = []\n",
    "    samples_per_class = size // 100  # Number of samples for each class in the chunk\n",
    "    for i in range(100):\n",
    "        # Append the correct number of samples from each class\n",
    "        chunk.extend(data_by_class[i][:samples_per_class])\n",
    "        # Remove the samples from data_by_class\n",
    "        data_by_class[i] = data_by_class[i][samples_per_class:]\n",
    "    # Shuffle the chunk\n",
    "    np.random.shuffle(chunk)\n",
    "    chunks.append(np.array(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49995"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to ensure that each sample in the total training dataset is chosen at least one\n",
    "total = np.concatenate(chunks, axis=0)\n",
    "unique_samples = np.unique(total, axis=0)\n",
    "\n",
    "len(unique_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "\n",
    "# Function to get label proportions in a chunk\n",
    "def get_label_proportions(label_distribution, chunk_size):\n",
    "    proportions = {}\n",
    "    for label, count in label_distribution.items():\n",
    "        proportions[label] = count / chunk_size\n",
    "    return proportions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"10_chunks\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_folder = \"iid\"\n",
    "iid_folder = os.path.join(folder, iid_folder)\n",
    "if not os.path.exists(iid_folder):\n",
    "    os.makedirs(iid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid distribution\n",
    "\n",
    "chunk_info = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "\n",
    "    label_distribution = get_label_distribution(chunk)\n",
    "    chunk_size =len(chunk)\n",
    "    # save info\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{iid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunk  size                                 label_distribution  \\\n",
      "0      1  6300  {0: 63, 1: 63, 2: 63, 3: 63, 4: 63, 5: 63, 6: ...   \n",
      "1      2  6100  {0: 61, 1: 61, 2: 61, 3: 61, 4: 61, 5: 61, 6: ...   \n",
      "2      3  5700  {0: 57, 1: 57, 2: 57, 3: 57, 4: 57, 5: 57, 6: ...   \n",
      "3      4  5700  {0: 57, 1: 57, 2: 57, 3: 57, 4: 57, 5: 57, 6: ...   \n",
      "4      5  4900  {0: 49, 1: 49, 2: 49, 3: 49, 4: 49, 5: 49, 6: ...   \n",
      "5      6  4800  {0: 48, 1: 48, 2: 48, 3: 48, 4: 48, 5: 48, 6: ...   \n",
      "6      7  4700  {0: 47, 1: 47, 2: 47, 3: 47, 4: 47, 5: 47, 6: ...   \n",
      "7      8  4600  {0: 46, 1: 46, 2: 46, 3: 46, 4: 46, 5: 46, 6: ...   \n",
      "8      9  4500  {0: 45, 1: 45, 2: 45, 3: 45, 4: 45, 5: 45, 6: ...   \n",
      "9     10  2700  {0: 27, 1: 27, 2: 27, 3: 27, 4: 27, 5: 27, 6: ...   \n",
      "\n",
      "                                   label_proportions  \n",
      "0  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "1  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "2  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "3  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "4  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "5  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "6  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "7  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "8  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n",
      "9  {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01, 4: 0.01, ...  \n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "df = pd.DataFrame(chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv(f\"{iid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
