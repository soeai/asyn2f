{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data separately\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train from 4D to 2D array (number of samples, width*height*channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "# Reshape y_train to 1D array\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((x_train, y_train))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_with_overlap(data, num_chunks=10, min_chunk_size=0.2, max_chunk_size=0.3):\n",
    "    num_samples = len(data)\n",
    "    num_classes = 10  # cifar10 has 10 classes\n",
    "\n",
    "    # Select random chunk sizes that are multiples of the number of classes\n",
    "    chunk_sizes = [np.random.choice(np.arange(int(min_chunk_size * num_samples), \n",
    "                                              int(max_chunk_size * num_samples)+1, \n",
    "                                              num_classes)) for _ in range(num_chunks)]\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Split data by classes\n",
    "    data_by_class = {i: [] for i in range(num_classes)}\n",
    "    for row in data:\n",
    "        data_by_class[int(row[-1])].append(row)\n",
    "\n",
    "    # Ensure each sample is included at least once\n",
    "    for i in range(num_classes):\n",
    "        np.random.shuffle(data_by_class[i])\n",
    "\n",
    "    # Copy of the original class samples before popping\n",
    "    data_by_class_original = {i: list(data) for i, data in data_by_class.items()}\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        chunk = []\n",
    "        for _ in range(size // num_classes):\n",
    "            for c in range(num_classes):\n",
    "                if data_by_class[c]:  # if there are samples left\n",
    "                    chunk.append(data_by_class[c].pop())\n",
    "                else:  # if all samples of this class have been used, start reusing\n",
    "                    chunk.append(data_by_class_original[c][np.random.choice(len(data_by_class_original[c]))])\n",
    "        chunks.append(np.array(chunk))\n",
    "    return chunks\n",
    "\n",
    "# Split train_data into 10 chunks\n",
    "chunks = get_chunks_with_overlap(train_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to ensure that each sample in the total training dataset is chosen at least one\n",
    "\n",
    "total = np.concatenate(chunks, axis=0)\n",
    "unique_samples = np.unique(total, axis=0)\n",
    "\n",
    "len(unique_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(chunks):\n",
    "    overlaps = np.zeros((len(chunks), len(chunks)))\n",
    "    for i in range(len(chunks)):\n",
    "        for j in range(i + 1, len(chunks)):\n",
    "            set_i = set(tuple(row) for row in chunks[i])\n",
    "            set_j = set(tuple(row) for row in chunks[j])\n",
    "            intersection = set_i & set_j\n",
    "            union = set_i | set_j\n",
    "            overlap = len(intersection) / len(union)\n",
    "            overlaps[i, j] = overlap\n",
    "            overlaps[j, i] = overlap  # the overlap is symmetric\n",
    "    return overlaps\n",
    "overlaps = calculate_overlap(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pair (0,1) is: 0.0\n",
      " pair (0,2) is: 0.0\n",
      " pair (0,3) is: 0.0\n",
      " pair (0,4) is: 0.10311784228485954\n",
      " pair (0,5) is: 0.12491266593472403\n",
      " pair (0,6) is: 0.11426889317062727\n",
      " pair (0,7) is: 0.1270743466125349\n",
      " pair (0,8) is: 0.13056675361631492\n",
      " pair (0,9) is: 0.12787663107947805\n",
      " pair (1,2) is: 0.0\n",
      " pair (1,3) is: 0.0\n",
      " pair (1,4) is: 0.10132829074808597\n",
      " pair (1,5) is: 0.12891417264407617\n",
      " pair (1,6) is: 0.11741340959644106\n",
      " pair (1,7) is: 0.12270979981029405\n",
      " pair (1,8) is: 0.13796357459658393\n",
      " pair (1,9) is: 0.13679334467763282\n",
      " pair (2,3) is: 0.0\n",
      " pair (2,4) is: 0.11747982775199632\n",
      " pair (2,5) is: 0.14480021405636817\n",
      " pair (2,6) is: 0.1337878142309331\n",
      " pair (2,7) is: 0.14521377564855825\n",
      " pair (2,8) is: 0.15866809881847477\n",
      " pair (2,9) is: 0.15549828178694158\n",
      " pair (3,4) is: 0.09409850189058537\n",
      " pair (3,5) is: 0.12428446186375122\n",
      " pair (3,6) is: 0.10909191338930622\n",
      " pair (3,7) is: 0.11975173420956553\n",
      " pair (3,8) is: 0.1285483949772805\n",
      " pair (3,9) is: 0.13141734217636944\n",
      " pair (4,5) is: 0.13540395983860373\n",
      " pair (4,6) is: 0.12468728109676774\n",
      " pair (4,7) is: 0.13630088074268032\n",
      " pair (4,8) is: 0.14505770871693538\n",
      " pair (4,9) is: 0.14907365567103478\n",
      " pair (5,6) is: 0.11888842457166736\n",
      " pair (5,7) is: 0.12764484872454024\n",
      " pair (5,8) is: 0.14014462327968277\n",
      " pair (5,9) is: 0.13697224166083508\n",
      " pair (6,7) is: 0.11837081035214256\n",
      " pair (6,8) is: 0.12465963661567404\n",
      " pair (6,9) is: 0.1237410071942446\n",
      " pair (7,8) is: 0.14018824197133803\n",
      " pair (7,9) is: 0.13874940786357176\n",
      " pair (8,9) is: 0.15046389673255345\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunks)):\n",
    "    for j in range(i+1, len(chunks)):\n",
    "        print(f\" pair ({i},{j}) is: {overlaps[i,j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1: 10970 vs 10970 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 2: 11250 vs 11250 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 3: 14100 vs 14100 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 4: 10230 vs 10230 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 5: 14470 vs 12629 --> additional sample = [212, 182, 186, 163, 182, 190, 188, 175, 192, 171]\n",
      "additional sample list: [212 182 186 163 182 190 188 175 192 171]\n",
      "chunk 6: 13140 vs 11571 --> additional sample = [148, 145, 164, 148, 157, 167, 155, 170, 157, 158]\n",
      "additional sample list: [148 145 164 148 157 167 155 170 157 158]\n",
      "chunk 7: 10990 vs 9849 --> additional sample = [116, 106, 98, 110, 117, 120, 125, 112, 110, 127]\n",
      "additional sample list: [116 106  98 110 117 120 125 112 110 127]\n",
      "chunk 8: 12710 vs 11239 --> additional sample = [144, 151, 138, 152, 147, 172, 150, 152, 116, 149]\n",
      "additional sample list: [144 151 138 152 147 172 150 152 116 149]\n",
      "chunk 9: 14900 vs 12868 --> additional sample = [197, 208, 210, 211, 202, 198, 219, 196, 195, 196]\n",
      "additional sample list: [197 208 210 211 202 198 219 196 195 196]\n",
      "chunk 10: 14780 vs 12800 --> additional sample = [190, 212, 204, 204, 193, 190, 186, 203, 205, 193]\n",
      "additional sample list: [190 212 204 204 193 190 186 203 205 193]\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicate_sample_from_chunk(chunks):\n",
    "    unique_chunks = []\n",
    "    additional_samples = []\n",
    "    num_classes = 10  # cifar10 has 10 classes\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        unique_samples = np.unique(chunk, axis=0)\n",
    "        unique_chunks.append(unique_samples)\n",
    "\n",
    "        # Count the number of samples from each category in the original and unique chunks\n",
    "        original_counts = np.bincount(chunk[:,-1], minlength=num_classes)\n",
    "        unique_counts = np.bincount(unique_samples[:,-1], minlength=num_classes)\n",
    "\n",
    "        # Compute the additional_samples for each category as the difference between the original and unique counts\n",
    "        additional_sample = original_counts - unique_counts\n",
    "        additional_samples.append(additional_sample.tolist())\n",
    "        print(f\"chunk {i+1}: {len(chunk)} vs {len(unique_samples)} --> additional sample = {additional_sample.tolist()}\")\n",
    "        print(f\"additional sample list: {additional_sample}\")\n",
    "        \n",
    "    return zip(unique_chunks, additional_samples)\n",
    "\n",
    "requirement = remove_duplicate_sample_from_chunk(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10970, 3073)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = chunks[0]\n",
    "\n",
    "chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_chunks(requirement, chunks, subset_factor=2):\n",
    "    num_classes = 10  # cifar10 has 10 classes\n",
    "\n",
    "    for i, (unique_chunk, additional_samples) in enumerate(requirement):\n",
    "        # Create a set for faster membership tests\n",
    "        unique_set = set(map(tuple, unique_chunk))\n",
    "\n",
    "        # Convert the list of additional samples into a numpy array for easier manipulation\n",
    "        additional_samples = np.array(additional_samples)\n",
    "\n",
    "        while np.any(additional_samples > 0):\n",
    "            # Randomly select a chunk different from the current one\n",
    "            while True:\n",
    "                random_chunk_index = np.random.choice(len(chunks))\n",
    "                if random_chunk_index != i:\n",
    "                    break\n",
    "\n",
    "            random_chunk = chunks[random_chunk_index]\n",
    "            np.random.shuffle(random_chunk)\n",
    "\n",
    "            for sample in random_chunk:\n",
    "                # If sample is not already in unique_set and there are still missing samples for its category\n",
    "                if tuple(sample) not in unique_set and additional_samples[int(sample[-1])] > 0:\n",
    "                    # Add sample to unique_chunk\n",
    "                    unique_chunk = np.vstack([unique_chunk, sample])\n",
    "                    # Add sample to unique_set\n",
    "                    unique_set.add(tuple(sample))\n",
    "                    # Decrease the count of missing samples for this category\n",
    "                    additional_samples[int(sample[-1])] -= 1\n",
    "\n",
    "            # Replace the old chunk with the new one in the chunks list\n",
    "            chunks[i] = unique_chunk\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Use the fill_chunks function\n",
    "filled_chunks = fill_chunks(requirement, chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1: 10970 vs 10970 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 2: 11250 vs 11250 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 3: 14100 vs 14100 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 4: 10230 vs 10230 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 5: 14470 vs 14470 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 6: 13140 vs 13140 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 7: 10990 vs 10990 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 8: 12710 vs 12710 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 9: 14900 vs 14900 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 10: 14780 vs 14780 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "result = remove_duplicate_sample_from_chunk(filled_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10970, 3073)\n",
      "(11250, 3073)\n",
      "(14100, 3073)\n",
      "(10230, 3073)\n",
      "(14470, 3073)\n",
      "(13140, 3073)\n",
      "(10990, 3073)\n",
      "(12710, 3073)\n",
      "(14900, 3073)\n",
      "(14780, 3073)\n"
     ]
    }
   ],
   "source": [
    "# Unzip the result to get chunks and additional_samples separately\n",
    "chunks, _ = zip(*result)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "\n",
    "# Function to get label proportions in a chunk\n",
    "def get_label_proportions(label_distribution, chunk_size):\n",
    "    proportions = {}\n",
    "    for label, count in label_distribution.items():\n",
    "        proportions[label] = count / chunk_size\n",
    "    return proportions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"10_chunks_overlap\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_folder = \"iid\"\n",
    "iid_folder = os.path.join(folder, iid_folder)\n",
    "if not os.path.exists(iid_folder):\n",
    "    os.makedirs(iid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid distribution\n",
    "\n",
    "chunk_info = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "\n",
    "    label_distribution = get_label_distribution(chunk)\n",
    "    chunk_size =len(chunk)\n",
    "    # save info\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{iid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunk   size                                 label_distribution  \\\n",
      "0      1  10970  {0: 1097, 1: 1097, 2: 1097, 3: 1097, 4: 1097, ...   \n",
      "1      2  11250  {0: 1125, 1: 1125, 2: 1125, 3: 1125, 4: 1125, ...   \n",
      "2      3  14100  {0: 1410, 1: 1410, 2: 1410, 3: 1410, 4: 1410, ...   \n",
      "3      4  10230  {0: 1023, 1: 1023, 2: 1023, 3: 1023, 4: 1023, ...   \n",
      "4      5  14470  {0: 1447, 1: 1447, 2: 1447, 3: 1447, 4: 1447, ...   \n",
      "5      6  13140  {0: 1314, 1: 1314, 2: 1314, 3: 1314, 4: 1314, ...   \n",
      "6      7  10990  {0: 1099, 1: 1099, 2: 1099, 3: 1099, 4: 1099, ...   \n",
      "7      8  12710  {0: 1271, 1: 1271, 2: 1271, 3: 1271, 4: 1271, ...   \n",
      "8      9  14900  {0: 1490, 1: 1490, 2: 1490, 3: 1490, 4: 1490, ...   \n",
      "9     10  14780  {0: 1478, 1: 1478, 2: 1478, 3: 1478, 4: 1478, ...   \n",
      "\n",
      "                                   label_proportions  \n",
      "0  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "1  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "2  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "3  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "4  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "5  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "6  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "7  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "8  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "9  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "df = pd.DataFrame(chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv(f\"{iid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = chunks[0][5]\n",
    "# image = sample[:-1]\n",
    "# label = sample[-1]\n",
    "# image = image.reshape((32, 32, 3))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
