{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 06:35:32.516248: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-18 06:35:32.551561: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-18 06:35:33.251704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data separately\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train from 4D to 2D array (number of samples, width*height*channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "# Reshape y_train to 1D array\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((x_train, y_train))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6840, 6740, 6230, 6010, 5290, 4380, 4350, 3770, 3670, 2720],\n",
       " array([0.12024607, 0.10583159, 0.07340995, 0.08775413, 0.12479917,\n",
       "        0.13484228, 0.07549685, 0.05441033, 0.13699571, 0.08621391]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate proportions for 10 chunks using a normal distribution around the mean\n",
    "mean = 0.05\n",
    "std_dev = 0.015\n",
    "proportions = np.random.normal(mean, std_dev, 10)\n",
    "\n",
    "# Clip values to ensure they're in range [0,1] and normalize so they sum to 1\n",
    "proportions = np.clip(proportions, 0, 1)\n",
    "proportions /= np.sum(proportions)\n",
    "\n",
    "# Compute the chunk sizes based on proportions, and ensure they're multiples of 10\n",
    "chunk_sizes = [int(p * 50000) // 10 * 10 for p in proportions]\n",
    "\n",
    "# Adjust the last chunk size to ensure the total sum is 50000\n",
    "chunk_sizes[-1] = 50000 - sum(chunk_sizes[:-1])\n",
    "\n",
    "# Sorting in descending order using sorted()\n",
    "chunk_sizes = sorted(chunk_sizes, reverse=True)\n",
    "chunk_sizes, proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "3469 samples are not used.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# Check if all samples are used\u001b[39;00m\n\u001b[1;32m     30\u001b[0m unused_samples \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([\u001b[39mlen\u001b[39m(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m label_data_copy\u001b[39m.\u001b[39mvalues()])\n\u001b[0;32m---> 31\u001b[0m \u001b[39massert\u001b[39;00m unused_samples \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00munused_samples\u001b[39m}\u001b[39;00m\u001b[39m samples are not used.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[39m# Verify the chunk sizes\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks:\n",
      "\u001b[0;31mAssertionError\u001b[0m: 3469 samples are not used."
     ]
    }
   ],
   "source": [
    "# Categorize samples by label\n",
    "samples_by_label = {}\n",
    "for label in range(10):\n",
    "    samples_by_label[label] = train_data[train_data[:, -1] == label]\n",
    "\n",
    "def get_samples_for_chunk(label_data, chunk_size):\n",
    "    chunk = []\n",
    "    for label, samples in label_data.items():\n",
    "        # Use a normal distribution to get the proportion of samples for this label\n",
    "        prop = np.random.normal(0.095, 0.025) # mean at 9.5% and std deviation 2.5% to get a range ~7% to ~12%\n",
    "        prop = np.clip(prop, 0.085, 0.1) \n",
    "        num_samples = int(chunk_size * prop)\n",
    "        \n",
    "        # Extract these samples\n",
    "        chunk_samples = samples[:num_samples]\n",
    "        chunk.extend(chunk_samples)\n",
    "        \n",
    "        # Remove these samples from the label data\n",
    "        label_data[label] = samples[num_samples:]\n",
    "    return chunk, label_data\n",
    "\n",
    "chunks = []\n",
    "label_data_copy = samples_by_label.copy()\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    chunk, label_data_copy = get_samples_for_chunk(label_data_copy, size)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Check if all samples are used\n",
    "unused_samples = sum([len(samples) for samples in label_data_copy.values()])\n",
    "assert unused_samples == 0, f\"{unused_samples} samples are not used.\"\n",
    "\n",
    "# Verify the chunk sizes\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6483\n",
      "6404\n",
      "5758\n",
      "5589\n",
      "4832\n",
      "3990\n",
      "4050\n",
      "3531\n",
      "3443\n",
      "2451\n"
     ]
    }
   ],
   "source": [
    "# Verify the chunk sizes\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in label_data_copy.values():\n",
    "    # print(len(category))\n",
    "    chunks[-1].extend(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5920"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = []\n",
    "# start_index = 0\n",
    "# for size in chunk_sizes:\n",
    "#     chunk = train_data[start_index: start_index + size]\n",
    "#     chunks.append(chunk)\n",
    "#     start_index += size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to ensure that each sample in the total training dataset is chosen at least one\n",
    "total = np.concatenate(chunks, axis=0)\n",
    "unique_samples = np.unique(total, axis=0)\n",
    "\n",
    "len(unique_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3073, 0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0][0]), chunks[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "\n",
    "# Function to get label proportions in a chunk\n",
    "def get_label_proportions(label_distribution, chunk_size):\n",
    "    proportions = {}\n",
    "    for label, count in label_distribution.items():\n",
    "        proportions[label] = count / chunk_size\n",
    "    return proportions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"10_chunks\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_folder = \"non_iid\"\n",
    "iid_folder = os.path.join(folder, iid_folder)\n",
    "if not os.path.exists(iid_folder):\n",
    "    os.makedirs(iid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid distribution\n",
    "\n",
    "chunk_info = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk = np.array(chunk)    \n",
    "    label_distribution = get_label_distribution(chunk)\n",
    "    chunk_size =len(chunk)\n",
    "    # save info\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{iid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunk  size                                 label_distribution  \\\n",
      "0      1  6483  {0: 684, 1: 581, 2: 684, 3: 684, 4: 581, 5: 68...   \n",
      "1      2  6404  {0: 641, 1: 674, 2: 665, 3: 576, 4: 572, 5: 59...   \n",
      "2      3  5758  {0: 529, 1: 529, 2: 531, 3: 529, 4: 529, 5: 62...   \n",
      "3      4  5589  {0: 541, 1: 542, 2: 510, 3: 510, 4: 601, 5: 60...   \n",
      "4      5  4832  {0: 529, 1: 449, 2: 529, 3: 529, 4: 449, 5: 44...   \n",
      "5      6  3990  {0: 438, 1: 379, 2: 372, 3: 438, 4: 385, 5: 42...   \n",
      "6      7  4050  {0: 435, 1: 369, 2: 369, 3: 435, 4: 435, 5: 43...   \n",
      "7      8  3531  {0: 377, 1: 320, 2: 377, 3: 320, 4: 320, 5: 36...   \n",
      "8      9  3443  {0: 367, 1: 367, 2: 367, 3: 311, 4: 311, 5: 36...   \n",
      "9     10  5920  {0: 459, 1: 790, 2: 596, 3: 668, 4: 817, 5: 45...   \n",
      "\n",
      "                                   label_proportions  \n",
      "0  {0: 0.10550670985654789, 1: 0.0896190035477402...  \n",
      "1  {0: 0.10009369144284821, 1: 0.1052467207995003...  \n",
      "2  {0: 0.09187217783952761, 1: 0.0918721778395276...  \n",
      "3  {0: 0.0967972803721596, 1: 0.0969762032563965,...  \n",
      "4  {0: 0.10947847682119205, 1: 0.0929221854304635...  \n",
      "5  {0: 0.10977443609022557, 1: 0.0949874686716792...  \n",
      "6  {0: 0.10740740740740741, 1: 0.0911111111111111...  \n",
      "7  {0: 0.10676862078731238, 1: 0.0906258850184083...  \n",
      "8  {0: 0.10659308742375835, 1: 0.1065930874237583...  \n",
      "9  {0: 0.07753378378378378, 1: 0.1334459459459459...  \n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "df = pd.DataFrame(chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv(f\"{iid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
