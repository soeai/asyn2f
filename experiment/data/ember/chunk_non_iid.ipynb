{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (600000, 2381) -- y shape: (600000,)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "data_folder = \"/home/student02/thaile/working_with_ember_dataset/data\"\n",
    "for i in range(10):\n",
    "    X_chunk, y_chunk = load_data(f'{data_folder}/chunk_{i}.pickle')\n",
    "    X.append(X_chunk)\n",
    "    y.append(y_chunk)\n",
    "    # break\n",
    "\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.concatenate(y, axis=0)\n",
    "print('X shape:', X.shape, '-- y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599785"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to ensure that each sample in the total training dataset is chosen at least one\n",
    "unique_samples = np.unique(X, axis=0)\n",
    "\n",
    "len(unique_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((X, y))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(train_data)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104360, 99720, 93430, 87450, 79130, 77890, 58020]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate proportions for 10 chunks using a normal distribution around the mean\n",
    "mean = 0.05\n",
    "std_dev = 0.01\n",
    "proportions = np.random.normal(mean, std_dev, 7)\n",
    "\n",
    "# Clip values to ensure they're in range [0,1] and normalize so they sum to 1\n",
    "proportions = np.clip(proportions, 0, 1)\n",
    "proportions /= np.sum(proportions)\n",
    "\n",
    "# Compute the chunk sizes based on proportions, and ensure they're multiples of 10\n",
    "chunk_sizes = [int(p * data_size) // 10 * 10 for p in proportions]\n",
    "\n",
    "# Adjust the last chunk size to ensure the total sum is data_size\n",
    "chunk_sizes[-1] = data_size - sum(chunk_sizes[:-1])\n",
    "\n",
    "# Sorting in descending order using sorted()\n",
    "chunk_sizes = sorted(chunk_sizes, reverse=True)\n",
    "chunk_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17393333333333333,\n",
       " 0.1662,\n",
       " 0.15571666666666667,\n",
       " 0.14575,\n",
       " 0.13188333333333332,\n",
       " 0.12981666666666666,\n",
       " 0.0967]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_portions = [size / data_size for size in chunk_sizes]\n",
    "\n",
    "chunks_portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Segregate the train_data into two lists based on class labels\n",
    "# data_class_0 = train_data[train_data[:, -1] == 0]\n",
    "# data_class_1 = train_data[train_data[:, -1] == 1]\n",
    "\n",
    "# chunks = []\n",
    "\n",
    "# # 2. For each chunk size, generate a random fraction for imbalance and sample data\n",
    "# for size in chunk_sizes:\n",
    "#     # Random fraction for class 0 between 0.1 and 0.9 for example\n",
    "#     fraction_class_0 = np.random.uniform(0.1, 0.9)\n",
    "#     num_class_0 = int(size * fraction_class_0)  # fraction of the chunk size for class 0\n",
    "#     num_class_1 = size - num_class_0  # remaining data for class 1\n",
    "    \n",
    "#     # Handle case when available samples are less than required samples\n",
    "#     if num_class_0 > len(data_class_0):\n",
    "#         num_class_0 = len(data_class_0)\n",
    "#         num_class_1 = size - num_class_0\n",
    "#     if num_class_1 > len(data_class_1):\n",
    "#         num_class_1 = len(data_class_1)\n",
    "#         num_class_0 = size - num_class_1\n",
    "    \n",
    "#     # Sample without replacement\n",
    "#     chunk_class_0 = np.random.choice(len(data_class_0), num_class_0, replace=False)\n",
    "#     chunk_class_1 = np.random.choice(len(data_class_1), num_class_1, replace=False)\n",
    "\n",
    "#     # Append to chunks\n",
    "#     chunks.append(np.vstack((data_class_0[chunk_class_0], data_class_1[chunk_class_1])))\n",
    "\n",
    "#     # 3. Remove the sampled data from main segregated lists\n",
    "#     data_class_0 = np.delete(data_class_0, chunk_class_0, axis=0)\n",
    "#     data_class_1 = np.delete(data_class_1, chunk_class_1, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Segregate the train_data into two lists based on class labels\n",
    "# data_class_0 = train_data[train_data[:, -1] == 0]\n",
    "# data_class_1 = train_data[train_data[:, -1] == 1]\n",
    "\n",
    "# chunks = []\n",
    "\n",
    "# # 2. For each chunk size, generate a random fraction for imbalance and sample data\n",
    "# # Note: loop until len(chunk_sizes) - 1\n",
    "# for size in chunk_sizes[:-2]:\n",
    "#     # Random fraction for class 0 between 0.1 and 0.9 for example\n",
    "#     fraction_class_0 = np.random.uniform(0.1, 0.9)\n",
    "#     num_class_0 = int(size * fraction_class_0)  # fraction of the chunk size for class 0\n",
    "#     num_class_1 = size - num_class_0  # remaining data for class 1\n",
    "    \n",
    "#     # Handle case when available samples are less than required samples\n",
    "#     if num_class_0 > len(data_class_0):\n",
    "#         num_class_0 = len(data_class_0)\n",
    "#         num_class_1 = size - num_class_0\n",
    "#     if num_class_1 > len(data_class_1):\n",
    "#         num_class_1 = len(data_class_1)\n",
    "#         num_class_0 = size - num_class_1\n",
    "    \n",
    "#     # Sample without replacement\n",
    "#     chunk_class_0 = np.random.choice(len(data_class_0), num_class_0, replace=False)\n",
    "#     chunk_class_1 = np.random.choice(len(data_class_1), num_class_1, replace=False)\n",
    "\n",
    "#     # Append to chunks\n",
    "#     chunks.append(np.vstack((data_class_0[chunk_class_0], data_class_1[chunk_class_1])))\n",
    "\n",
    "#     # 3. Remove the sampled data from main segregated lists\n",
    "#     data_class_0 = np.delete(data_class_0, chunk_class_0, axis=0)\n",
    "#     data_class_1 = np.delete(data_class_1, chunk_class_1, axis=0)\n",
    "\n",
    "# # 4. The last chunk takes all remaining values\n",
    "# chunks.append(np.vstack((data_class_0, data_class_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Segregate the train_data into two lists based on class labels\n",
    "data_class_0 = train_data[train_data[:, -1] == 0]\n",
    "data_class_1 = train_data[train_data[:, -1] == 1]\n",
    "\n",
    "chunks = []\n",
    "\n",
    "# 2. Process until len(chunk_sizes) - 3\n",
    "for size in chunk_sizes[:-3]:\n",
    "    # Random fraction for class 0 between 0.1 and 0.9 for example\n",
    "    fraction_class_0 = np.random.uniform(0.1, 0.9)\n",
    "    num_class_0 = int(size * fraction_class_0)\n",
    "    num_class_1 = size - num_class_0\n",
    "    \n",
    "    # Sample without replacement\n",
    "    chunk_class_0 = np.random.choice(len(data_class_0), num_class_0, replace=False)\n",
    "    chunk_class_1 = np.random.choice(len(data_class_1), num_class_1, replace=False)\n",
    "\n",
    "    # Append to chunks\n",
    "    chunks.append(np.vstack((data_class_0[chunk_class_0], data_class_1[chunk_class_1])))\n",
    "\n",
    "    # Remove the sampled data from main segregated lists\n",
    "    data_class_0 = np.delete(data_class_0, chunk_class_0, axis=0)\n",
    "    data_class_1 = np.delete(data_class_1, chunk_class_1, axis=0)\n",
    "\n",
    "# 3. For the last three chunks, combine, shuffle, and split the data\n",
    "combined_data = np.vstack((data_class_0, data_class_1))\n",
    "np.random.shuffle(combined_data)\n",
    "\n",
    "# Split according to the sizes of the last three chunks\n",
    "split_index1 = chunk_sizes[-3]  # Size of the third last chunk\n",
    "split_index2 = split_index1 + chunk_sizes[-2]  # Size of the second last chunk\n",
    "chunks.append(combined_data[:split_index1])\n",
    "chunks.append(combined_data[split_index1:split_index2])\n",
    "chunks.append(combined_data[split_index2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "\n",
    "# Function to get label proportions in a chunk\n",
    "def get_label_proportions(label_distribution, chunk_size):\n",
    "    proportions = {}\n",
    "    for label, count in label_distribution.items():\n",
    "        proportions[label] = count / chunk_size\n",
    "    return proportions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"./7_chunks\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "folder = \"non_iid\"\n",
    "folder = os.path.join(folder, folder)\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid distribution\n",
    "\n",
    "chunk_info = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "\n",
    "    label_distribution = get_label_distribution(chunk)\n",
    "    chunk_size =len(chunk)\n",
    "    # save info\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunk    size        label_distribution  \\\n",
      "0      1  104360  {0.0: 86162, 1.0: 18198}   \n",
      "1      2   99720  {0.0: 19279, 1.0: 80441}   \n",
      "2      3   93430  {0.0: 23448, 1.0: 69982}   \n",
      "3      4   87450  {0.0: 44198, 1.0: 43252}   \n",
      "4      5   79130  {0.0: 46820, 1.0: 32310}   \n",
      "5      6   77890  {0.0: 46008, 1.0: 31882}   \n",
      "6      7   58020  {0.0: 34085, 1.0: 23935}   \n",
      "\n",
      "                                   label_proportions  \n",
      "0  {0.0: 0.8256228440015332, 1.0: 0.1743771559984...  \n",
      "1  {0.0: 0.1933313277176093, 1.0: 0.8066686722823...  \n",
      "2  {0.0: 0.25096863962324734, 1.0: 0.749031360376...  \n",
      "3  {0.0: 0.5054088050314466, 1.0: 0.4945911949685...  \n",
      "4  {0.0: 0.5916845696954379, 1.0: 0.4083154303045...  \n",
      "5  {0.0: 0.5906791629220696, 1.0: 0.4093208370779...  \n",
      "6  {0.0: 0.5874698379869011, 1.0: 0.4125301620130...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "df = pd.DataFrame(chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{folder}/chunks_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
